{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e43a772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from itertools import combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cb38749e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gini(counts):\n",
    "    \"\"\" Calculates Gini Impurity \n",
    "    \n",
    "        Args:\n",
    "            counts: The number of samples in each class.\n",
    "    \"\"\"\n",
    "    # counts: array of shape (K,)\n",
    "    total = counts.sum()\n",
    "    if total == 0:\n",
    "        return 0.0\n",
    "    p = counts / total\n",
    "    return 1.0 - np.sum(p * p)\n",
    "\n",
    "def entropy(counts):\n",
    "    \"\"\" Calculate Entropy\n",
    "    \n",
    "        Args:\n",
    "            counts: The number of samples in each class.\n",
    "    \"\"\"\n",
    "    p = counts/counts.sum()\n",
    "    p = p[p > 0]   # avoid log(0)\n",
    "    return -np.sum(p * np.log2(p))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f47c1933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(np.float64(7.0), np.float64(0.30000000000000004))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def best_split_one_feature(x, y):\n",
    "    x = np.asarray(x)\n",
    "    y = np.asarray(y)\n",
    "\n",
    "    # Sort by feature to scan thresholds once\n",
    "    order = np.argsort(x, kind=\"mergesort\")\n",
    "    x = x[order]\n",
    "    classes, y_enc = np.unique(y[order], return_inverse=True)   # Encode labels to 0...K-1\n",
    "    K = len(classes)\n",
    "    N = len(x)\n",
    "    if N <= 1:\n",
    "        return None, np.inf  # no split\n",
    "\n",
    "    parent_counts = np.bincount(y_enc, minlength=K)\n",
    "    parent_imp = gini(parent_counts)\n",
    "\n",
    "    # Prefix sums of class counts\n",
    "    prefix = np.zeros((N+1, K), dtype=int)\n",
    "    for i in range(1, N+1):\n",
    "        prefix[i] = prefix[i-1]\n",
    "        prefix[i, y_enc[i-1]] += 1\n",
    "    \n",
    "    best_t = None\n",
    "    best_after_imp = np.inf  # we minimize weighted impurity after\n",
    "\n",
    "    for i in range(1, N):\n",
    "        # Skip if they are the same\n",
    "        if x[i-1]==x[i]:\n",
    "            continue\n",
    "\n",
    "        t = (x[i-1] + x[i]) / 2   # The threshold is the middle point\n",
    "        \n",
    "        n_left = i\n",
    "        n_right = N - i\n",
    "        left_counts = prefix[i]\n",
    "        right_counts = parent_counts - left_counts\n",
    "\n",
    "        if n_left==0 or n_right==0:\n",
    "            continue\n",
    "\n",
    "        left_imp = gini(left_counts)\n",
    "        right_imp = gini(right_counts)\n",
    "        weighted_after = (n_left/N) * left_imp + (n_right/N) * right_imp\n",
    "\n",
    "        if weighted_after < best_after_imp:\n",
    "            best_after_imp = weighted_after\n",
    "            best_t = t\n",
    "    \n",
    "    return best_t, best_after_imp\n",
    "\n",
    "x=[1, 3, 5, 6, 8]\n",
    "y=[0, 1, 0, 0, 2]\n",
    "\n",
    "best_split_one_feature(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f61aef33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, np.float64(3.5), np.float64(0.0))\n"
     ]
    }
   ],
   "source": [
    "def best_split(X, y):\n",
    "    X = np.asarray(X)\n",
    "    y = np.asarray(y)\n",
    "    N, D = X.shape  # N samples, D features\n",
    "    \n",
    "    best_feat = None\n",
    "    best_t = None\n",
    "    best_imp = np.inf\n",
    "    for i in range(D):\n",
    "        x_i = X[:,i]  # get feature i (column i of the matrix)\n",
    "        t, imp = best_split_one_feature(x_i, y)\n",
    "\n",
    "        if imp < best_imp:\n",
    "            best_imp = imp\n",
    "            best_feat = i\n",
    "            best_t = t\n",
    "    return best_feat, best_t, best_imp\n",
    "\n",
    "\n",
    "X = [\n",
    "    [1, 6],\n",
    "    [2, 4],\n",
    "    [3, 3],\n",
    "    [1, 0]\n",
    "]\n",
    "\n",
    "y = [0, 0, 1, 1]\n",
    "\n",
    "print(best_split(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f9b84b5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Optional, Tuple, Union, List\n",
    "@dataclass\n",
    "class Node:\n",
    "    feature:Optional[int] = None\n",
    "    threshold: Optional[float] = None\n",
    "    left: Optional[\"Node\"] = None\n",
    "    right: Optional[\"Node\"] = None\n",
    "\n",
    "    # for the leaves\n",
    "    proba: Optional[np.ndarray] = None\n",
    "    label: Optional[int] = None\n",
    "\n",
    "    depth: int = 0\n",
    "    n_samples: int = 0\n",
    "    impurity: float = 0.0\n",
    "\n",
    "    is_categorical = False\n",
    "\n",
    "    def is_leaf(self) -> bool:\n",
    "        return self.proba is not None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7b4dc627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecisionTreeClassifier:\n",
    "    def __init__(self, *, criterion: str = \"gini\", max_depth: Optional[int] = None,\n",
    "                min_samples_split: int = 2, min_samples_leaf: int = 1, \n",
    "                min_impurity_decrease: float = 0.0\n",
    "    ):\n",
    "        \"\"\"CART Decision Tree classifier\n",
    "        Args:\n",
    "            criterion: 'gini' or 'entropy'.\n",
    "            max_depth: maximum depth of the tree (root=0). None => unlimited.\n",
    "            min_samples_split: minimum samples to consider splitting a node.\n",
    "            min_samples_leaf: minimum samples required at any child node.\n",
    "            min_impurity_decrease: required impurity decrease to accept a split.\n",
    "        \"\"\"\n",
    "        if criterion not in {\"gini\", \"entropy\"}:\n",
    "            raise ValueError(\"criterion must be 'gini' or 'entropy'\")\n",
    "        self.criterion = criterion\n",
    "        self.max_depth = max_depth\n",
    "        self.min_samples_split = min_samples_split\n",
    "        self.min_samples_leaf = min_samples_leaf\n",
    "        self.min_impurity_decrease = min_impurity_decrease\n",
    "\n",
    "        # Set during fit\n",
    "        self.n_classes = None\n",
    "        self.n_features = None\n",
    "        self.tree = None\n",
    "        self.classes = None\n",
    "\n",
    "    def fit(self, X: np.array, y: np.array) -> \"DecisionTreeClassifier\":\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        if X.ndim != 2: raise ValueError(\"X must be 2D\")\n",
    "        if y.ndim != 1: raise ValueError(\"y must be 1D\")\n",
    "        if X.shape[0] != y.shape[0]: raise ValueError(\"rows mismatch\")\n",
    "\n",
    "        classes, y_enc = np.unique(y, return_inverse=True)\n",
    "        self.classes = classes\n",
    "        self.n_classes = len(classes)\n",
    "        self.n_features = X.shape[1]\n",
    "\n",
    "        idx = np.arange(X.shape[0])\n",
    "        self.tree = self._build(X, y_enc, idx, depth=0)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X: np.array) -> np.array:\n",
    "        proba = self.predict_proba(X)\n",
    "        pred_idx = np.argmax(proba, axis=1)\n",
    "        return self.classes[pred_idx]\n",
    "    \n",
    "    def predict_proba(self, X: np.array) -> np.array:\n",
    "        if self.tree is None: raise RuntimeError(\"Model is not fitted\")\n",
    "        X = np.asarray(X)\n",
    "        out = np.zeros(shape=(X.shape[0], self.n_classes), dtype=float)\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            head = self.tree\n",
    "\n",
    "            while not head.is_leaf():\n",
    "                feature = head.feature\n",
    "                # For categorical nodes use in operator\n",
    "                if head.is_categorical:\n",
    "                    if X[i,feature] in head.threshold:\n",
    "                        head = head.left\n",
    "                    else:\n",
    "                        head = head.right\n",
    "                # For numerical nodes use < operator\n",
    "                else:\n",
    "                    if X[i,feature] < head.threshold:\n",
    "                        head = head.left\n",
    "                    else:\n",
    "                        head = head.right\n",
    "            out[i] = head.proba\n",
    "        \n",
    "        return out\n",
    "    \n",
    "    def _gini(self, counts: np.array) -> float:\n",
    "        \"\"\" Calculates Gini Impurity \n",
    "        \n",
    "            Args:\n",
    "                counts: The number of samples in each class.\n",
    "        \"\"\"\n",
    "        # counts: array of shape (K,)\n",
    "        total = counts.sum()\n",
    "        if total == 0:\n",
    "            return 0.0\n",
    "        p = counts / total\n",
    "        return 1.0 - np.sum(p * p)\n",
    "\n",
    "    def _entropy(self, counts: np.array) -> float:\n",
    "        \"\"\" Calculate Entropy\n",
    "        \n",
    "            Args:\n",
    "                counts: The number of samples in each class.\n",
    "        \"\"\"\n",
    "        total = counts.sum()\n",
    "        if total == 0.0: return 0.0 # check it's not empty\n",
    "        p = counts/total\n",
    "        p = p[p > 0]   # avoid log(0)\n",
    "        return -np.sum(p * np.log2(p))\n",
    "\n",
    "    def _impurity(self, counts: np.array):\n",
    "        if self.criterion==\"gini\":\n",
    "            return self._gini(counts)\n",
    "        elif self.criterion==\"entropy\":\n",
    "            return self._entropy(counts)\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def _best_split_one_feature_numerical(self, x, y):\n",
    "        x = np.asarray(x)\n",
    "        y = np.asarray(y)\n",
    "\n",
    "        # Sort by feature to scan thresholds once\n",
    "        order = np.argsort(x, kind=\"mergesort\")\n",
    "        x = x[order]\n",
    "        classes, y_enc = np.unique(y[order], return_inverse=True)   # Encode labels to 0...K-1\n",
    "        K = len(classes)\n",
    "        N = len(x)\n",
    "        if N <= 1:\n",
    "            return None, np.inf  # no split\n",
    "\n",
    "        parent_counts = np.bincount(y_enc, minlength=K)\n",
    "        parent_imp = self._impurity(parent_counts)\n",
    "\n",
    "        # Prefix sums of class counts\n",
    "        prefix = np.zeros((N+1, K), dtype=int)\n",
    "        for i in range(1, N+1):\n",
    "            prefix[i] = prefix[i-1]\n",
    "            prefix[i, y_enc[i-1]] += 1\n",
    "        \n",
    "        best_t = None\n",
    "        best_after_imp = np.inf  # we minimize weighted impurity after\n",
    "\n",
    "        for i in range(1, N):\n",
    "            # Skip if they are the same\n",
    "            if x[i-1]==x[i]:\n",
    "                continue\n",
    "\n",
    "            t = (x[i-1] + x[i]) / 2   # The threshold is the middle point\n",
    "            \n",
    "            n_left = i\n",
    "            n_right = N - i\n",
    "            left_counts = prefix[i]\n",
    "            right_counts = parent_counts - left_counts\n",
    "\n",
    "            if n_left==0 or n_right==0:\n",
    "                continue\n",
    "\n",
    "            left_imp = self._impurity(left_counts)\n",
    "            right_imp = self._impurity(right_counts)\n",
    "            weighted_after = (n_left/N) * left_imp + (n_right/N) * right_imp\n",
    "\n",
    "            if weighted_after < best_after_imp:\n",
    "                best_after_imp = weighted_after\n",
    "                best_t = t\n",
    "        \n",
    "        return best_t, best_after_imp\n",
    "    \n",
    "    def _best_split_one_feature_categorical(self, x, y):\n",
    "        cats, x_enc = np.unique(x, return_inverse=True)\n",
    "        \n",
    "        K = len(cats)\n",
    "        if K <= 1:\n",
    "            return None, np.inf # No split possible\n",
    "        \n",
    "        parent_counts = np.bincount(y, minlength=self.n_classes)\n",
    "        parent_imp = self._impurity(parent_counts)\n",
    "\n",
    "        best_subset = None\n",
    "        best_after_imp = np.inf\n",
    "\n",
    "        for r in range(1, K):\n",
    "            for subset in combinations(range(K), r):\n",
    "                mask_left = np.isin(x_enc, subset)\n",
    "                left_counts = np.bincount(y[mask_left], minlength=self.n_classes)\n",
    "                right_counts = np.bincount(y[~mask_left], minlength=self.n_classes)\n",
    "\n",
    "                n_left = mask_left.sum()\n",
    "                n_right = (~mask_left).sum()\n",
    "\n",
    "                if n_left < self.min_samples_leaf or n_right < self.min_samples_leaf:\n",
    "                    continue\n",
    "                \n",
    "                left_imp = self._impurity(left_counts)\n",
    "                right_imp = self._impurity(right_counts)\n",
    "                N = n_left + n_right\n",
    "                weighted_after = (n_left/N) * left_imp + (n_right/N) * right_imp\n",
    "\n",
    "                if weighted_after < best_after_imp:\n",
    "                    best_after_imp = weighted_after\n",
    "                    best_subset = subset\n",
    "        \n",
    "        if best_subset is None:\n",
    "            return None, np.inf\n",
    "\n",
    "        return cats[list(best_subset)], best_after_imp\n",
    "    \n",
    "    def _best_split_one_feature(self, x, y):\n",
    "        if np.issubdtype(x.dtype, np.number):   # If it's a numerical feature we use the numerical split\n",
    "            return self._best_split_one_feature_numerical(x, y)\n",
    "        else:                                   # If it's a categorical feature we use the categorical split\n",
    "            return self._best_split_one_feature_categorical(x, y)\n",
    "    \n",
    "    def _best_split(self, X, y):\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        N, D = X.shape  # N samples, D features\n",
    "        \n",
    "        best_feat = None\n",
    "        best_t = None\n",
    "        best_imp = np.inf\n",
    "        for i in range(D):\n",
    "            x_i = X[:,i]  # get feature i (column i of the matrix)\n",
    "            t, imp = self._best_split_one_feature(x_i, y)\n",
    "            if t is None:\n",
    "                continue\n",
    "\n",
    "            if imp < best_imp:\n",
    "                best_imp = imp\n",
    "                best_feat = i\n",
    "                best_t = t\n",
    "        return best_feat, best_t, best_imp\n",
    "    \n",
    "    def _build(self, X: np.array, y: np.array, idx: np.array, depth: int=0) -> Node:\n",
    "        node = Node(depth=depth, n_samples=idx.size)\n",
    "        \n",
    "        counts = np.bincount(y[idx], minlength=self.n_classes)\n",
    "        proba = counts / counts.sum()\n",
    "        imp = self._impurity(counts)\n",
    "        node.impurity = imp\n",
    "\n",
    "        # Stopping criteria\n",
    "        if (\n",
    "            (self.max_depth is not None and depth >= self.max_depth)\n",
    "            or (idx.size < self.min_samples_split)\n",
    "            or (counts.max() == counts.sum())   # pure node\n",
    "        ):\n",
    "            node.proba = proba\n",
    "            node.label = int(np.argmax(proba))\n",
    "            return node\n",
    "        \n",
    "        # Find best split\n",
    "        best_feat, best_t, best_imp = self._best_split(X[idx], y[idx])\n",
    "        gain = imp - best_imp\n",
    "\n",
    "        if best_feat is None or gain < self.min_impurity_decrease:   # No valid split on any feature or gain smaller than min decrease\n",
    "            node.proba = proba\n",
    "            node.label = int(np.argmax(proba))\n",
    "            return node\n",
    "        \n",
    "        # Partition indices\n",
    "        if np.issubdtype(X[:, best_feat].dtype, np.number):\n",
    "            mask_left = X[idx, best_feat] <= best_t\n",
    "        else:\n",
    "            mask_left = np.isin(X[idx, best_feat], best_t)  # if the feature was categorical the threshold will be a subset of categories returned by the best_split method\n",
    "            node.is_categorical = True                      # Set the is_categorical attribute of the node to True\n",
    "        \n",
    "        left_idx = idx[mask_left]\n",
    "        right_idx = idx[~mask_left]\n",
    "        \n",
    "        # Check min_samples_leaf\n",
    "        if (left_idx.size < self.min_samples_leaf) or (right_idx.size < self.min_samples_leaf):\n",
    "            node.proba = proba\n",
    "            node.label = int(np.argmax(proba))\n",
    "            return node\n",
    "        \n",
    "        left_node = self._build(X, y, left_idx, depth+1)\n",
    "        right_node = self._build(X, y, right_idx, depth+1)\n",
    "        \n",
    "        node.feature = int(best_feat)\n",
    "        # node.threshold = float(best_t)\n",
    "        node.threshold = best_t\n",
    "        node.left = left_node\n",
    "        node.right = right_node\n",
    "        return node\n",
    "    \n",
    "    def print_tree(self):\n",
    "        if self.tree is None: raise RuntimeError(\"Model is not fitted\")\n",
    "        self._print(self.tree)\n",
    "\n",
    "    def _print(self, node, indent=\"\"):\n",
    "        if node.is_leaf():\n",
    "            print(f\"{indent}Leaf(depth={node.depth}, n={node.n_samples}, proba={np.round(node.proba, 3)})\")\n",
    "        elif node.is_categorical:\n",
    "            print(f\"{indent}[X{node.feature} in {node.threshold}] imp={node.impurity:.3f}\")\n",
    "        else:\n",
    "            print(f\"{indent}[X{node.feature} < {node.threshold:.3f}] imp={node.impurity:.3f}\")\n",
    "            self._print(node.left, indent+\"  \")\n",
    "            self._print(node.right, indent+\"  \")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15bb296",
   "metadata": {},
   "source": [
    "## Sanity Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f5eec653",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[0.],[1.],[2.]])\n",
    "y = np.array([1,1,1])\n",
    "clf = DecisionTreeClassifier().fit(X,y)\n",
    "assert clf.tree.is_leaf() and clf.predict(X).tolist() == [1,1,1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "831a54da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t, imp:   (np.float64(0.5), np.float64(0.5))\n",
      "t, imp:   (np.float64(0.5), np.float64(0.5))\n",
      "t, imp:   (np.float64(0.5), np.float64(0.5))\n",
      "t, imp:   (np.float64(0.5), np.float64(0.5))\n",
      "t, imp:   (None, inf)\n",
      "t, imp:   (np.float64(0.5), np.float64(0.0))\n",
      "t, imp:   (None, inf)\n",
      "t, imp:   (np.float64(0.5), np.float64(0.0))\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([0,1,1,0])\n",
    "acc1 = (DecisionTreeClassifier(max_depth=1).fit(X,y).predict(X) == y).mean()\n",
    "acc3 = (DecisionTreeClassifier(max_depth=3).fit(X,y).predict(X) == y).mean()\n",
    "assert acc3 > acc1 + 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2b88f3fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t, imp:   (np.float64(5.1), np.float64(0.0))\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[0.0],[0.1],[0.2],[10.0]])\n",
    "y = np.array([0,0,0,1])\n",
    "clf = DecisionTreeClassifier(min_samples_leaf=2).fit(X,y)\n",
    "assert clf.tree.is_leaf()  # split that isolates the outlier is rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7772079a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t, imp:   (np.float64(0.5), np.float64(0.5))\n",
      "t, imp:   (np.float64(0.5), np.float64(0.5))\n",
      "t, imp:   (None, inf)\n",
      "t, imp:   (np.float64(0.5), np.float64(0.0))\n",
      "t, imp:   (None, inf)\n",
      "t, imp:   (np.float64(0.5), np.float64(0.0))\n",
      "[X0 < 0.500] imp=0.500\n",
      "  [X1 < 0.500] imp=0.500\n",
      "    Leaf(depth=2, n=1, proba=[1. 0.])\n",
      "    Leaf(depth=2, n=1, proba=[0. 1.])\n",
      "  [X1 < 0.500] imp=0.500\n",
      "    Leaf(depth=2, n=1, proba=[0. 1.])\n",
      "    Leaf(depth=2, n=1, proba=[1. 0.])\n"
     ]
    }
   ],
   "source": [
    "# Trying the print method\n",
    "X = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "y = np.array([0,1,1,0])\n",
    "clf = DecisionTreeClassifier(max_depth=3).fit(X, y)\n",
    "clf.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "32fe4ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: [1 1 1]\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[\"red\"], [\"red\"], [\"red\"]])\n",
    "y = np.array([1, 1, 1])\n",
    "\n",
    "clf = DecisionTreeClassifier().fit(X, y)\n",
    "print(\"Pred:\", clf.predict(X))\n",
    "assert clf.tree.is_leaf()\n",
    "assert clf.predict(X).tolist() == [1, 1, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "84e2e223",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred: [0 0 1 1]\n",
      "Feature: 0 Threshold: ['blue']\n"
     ]
    }
   ],
   "source": [
    "X = np.array([[\"red\"], [\"red\"], [\"blue\"], [\"blue\"]])\n",
    "y = np.array([0, 0, 1, 1])\n",
    "\n",
    "clf = DecisionTreeClassifier().fit(X, y)\n",
    "print(\"Pred:\", clf.predict(X))\n",
    "print(\"Feature:\", clf.tree.feature, \"Threshold:\", clf.tree.threshold)\n",
    "\n",
    "assert not clf.tree.is_leaf()\n",
    "assert set(clf.tree.threshold) == {\"red\"} or set(clf.tree.threshold) == {\"blue\"}  # best subset\n",
    "assert clf.predict(X).tolist() == [0, 0, 1, 1]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
